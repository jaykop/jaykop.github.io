---
title: "Cache"
classes: wide
categories: 
  - post
  - OS
sidebar:
  nav: "main"
author_profile: true
---
   
## 캐시 Cache
* CPU가 빠른 속도가 데이터를 주고 받을 수 있도록 도와주는 메모리
* Locality of Reference 원리에 따라 메모리 저장소의 데이터를 미리 가져와 보관

### 캐시가 빠른 이유
1. 참조 지역성의 원리
  * 자주 사용하는 데이터를 우선적으로 저장
2. 유효 비트와 태그 필드를 이용한 메모리 접근
  * 시간 복잡도 O(1)에 수렴

### 참조 지역성 Locality of Reference의 원리
* 시간 지역성
  * 최근에 접근한 데이터에 다시 접근할 가능성이 높다
  * ex 1) 루프는 동일한 명령을 반복
  * ex 2) 함수 호출 및 반환은 스택 메모리에 반복적인 엑세스가 발생
* 공간 지역성
  * 접근한 데이터에 가까운 주소에 접근할 가능성이 높다
  * 배열을 처리할 때 등 순차적인 방식으로 메모리에 접근한다

## 캐시의 구조

![image](/assets/images/cachs set.png)  

* 캐시 메모리는 S개의 Cache Set으로 구성
* 각 Set은 E개의 Cache Entry를 가지고 있는 캐시 라인으로 구성

![image](/assets/images/cache entry.png)  

### 캐시 블록 Cache Block
* 케시 메모리의 데이터 그룹 단위
* 데이터를 가지고 있다

### 캐시 태그 Cache Tag
* 캐리 블록 고유 식별값
  * CPU는 태그 값을 통해 캐시 블록에 접근한다
* 캐시 블록에 올바른 데이터가 저장되어 있는지 Valid bit로 확인
  * 비어있거나 비정상적인 값을 가지고 있으면 유효비트는 0으로 set 되어 유효한 블록이 없다고 알려준다

### 캐시 오버헤드 예시
* 캐시 비트는 캐시 메모리 크기에서 포함되어 있지 않는 오버헤드다
  * 태그 오버헤드가 증가할수록 데이터 엑스사에 대한 레이턴시가 증가한다
  * 이를 줄이기 위해 CPU는 태그를 확인하는 과정과 데이터에 접근하는 과정을 동시에 수행한다

* 32KB 캐시가 있다고 가정
* 캐시 블록 크기는 32 바이트
* 태그 필드로 17비트 + 유효 비트 1비트 = 캐시 태그 18 비트
* 32KB는 = 32바이트 x 1024개
  * 18 비트 x 1024 = 18Kb (키로비트) -> 2.25 KB(키로바이트)
* 32KB의 캐시 메모리는 실제로 34.25KB의 크기다

## 캐시 메모리 관리 방법
* OS가 페이지Page라는 단위로 자주 사용하는 부분과 아닌 부분을 나눠 관리
* Pre-fetch라는 기능을 활용
  * 하부 단계의 메모리에서 데이터나 명령을 미리 불러오는 것(?)
  * 메모리 레이턴시를 줄이고 하드웨어의 성능을 쥐어짜낸다
  * data prefetch 혹은 instruction prefetch

## 코드를 최적화하는 방법
### 코드 파티셔닝

### 인라이닝

### 정렬


## 출처
* <https://blog.naver.com/PostView.nhn?blogId=cjsksk3113&logNo=222290234374&parentCategoryNo=&categoryNo=&viewDate=&isShowPopularPosts=false&from=postView>
* <https://blog.naver.com/cjsksk3113/222251293739>
* <https://en.wikipedia.org/wiki/Cache_prefetching>
* <https://blog.naver.com/cjsksk3113/222282661350>